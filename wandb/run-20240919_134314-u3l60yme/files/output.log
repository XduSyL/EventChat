  0%|                                                                                                         | 0/772 [00:00<?, ?it/s]/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
{'loss': 2.6337, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|                                                                                               | 1/772 [00:20<4:26:22, 20.73s/it]


  0%|▎                                                                                              | 3/772 [01:00<4:16:11, 19.99s/it]
{'loss': 2.5832, 'learning_rate': 2.5e-05, 'epoch': 0.02}


  1%|▌                                                                                              | 5/772 [01:38<4:08:19, 19.43s/it]
{'loss': 2.2113, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.03}
LoRA 参数已保存到 /data/SyL/Event_RGB/checkpoints/llava-v1.5-7b-instruction/tmp-checkpoint-5/lora_parameters.pth
视觉投影器参数已保存到 /data/SyL/Event_RGB/checkpoints/llava-v1.5-7b-instruction/tmp-checkpoint-5/visual_projecotor_parameters.pth
  1%|▌                                                                                              | 5/772 [01:38<4:08:19, 19.43s/it]Traceback (most recent call last):
  File "/data/SyL/Event_RGB/deepspeed_train.py", line 126, in <module>
    trainer.train()
  File "/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 2302, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 2378, in _save_checkpoint
    self.save_model(staging_output_dir, _internal_call=True)
  File "/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 2873, in save_model
    self._save(output_dir, state_dict=state_dict)
  File "/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py", line 2958, in _save
    self.model.save_pretrained(
  File "/data/SyL/Event_RGB/model/EventChatModel.py", line 187, in save_pretrained
    self.save_model(save_directory)
  File "/data/SyL/Event_RGB/model/EventChatModel.py", line 170, in save_model
    json.dump(peft_config, f, indent=4)
  File "/home/user/anaconda3/envs/llava/lib/python3.10/json/__init__.py", line 179, in dump
    for chunk in iterable:
  File "/home/user/anaconda3/envs/llava/lib/python3.10/json/encoder.py", line 431, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File "/home/user/anaconda3/envs/llava/lib/python3.10/json/encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "/home/user/anaconda3/envs/llava/lib/python3.10/json/encoder.py", line 438, in _iterencode
    o = _default(o)
  File "/home/user/anaconda3/envs/llava/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type LoraConfig is not JSON serializable